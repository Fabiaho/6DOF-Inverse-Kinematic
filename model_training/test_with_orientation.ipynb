{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules.modules import HyperNet, MainNet\n",
    "import ikpy.chain\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime Configuration defaults\n",
    "class Config:\n",
    "    chain_path='assets/UR5/urdf/ur5_robot.urdf'\n",
    "    train_data_path='data/ur5/ur5_train_data.csv'\n",
    "    test_data_path='data/ur5/ur5_test_data.csv'\n",
    "    num_joints=6\n",
    "    lr=0.001\n",
    "    num_epochs=200\n",
    "    num_solutions_validation=10\n",
    "    batch_size=2048\n",
    "    early_stopping_epochs=50\n",
    "    grad_clip=1\n",
    "    embedding_dim=128\n",
    "    hypernet_input_dim=6\n",
    "    hypernet_hidden_size=1024\n",
    "    hypernet_num_hidden_layers=3\n",
    "    jointnet_hidden_size=256\n",
    "    num_gaussians=50\n",
    "    exp_dir=\"runs\"\n",
    "    jointnet_output_dim=150\n",
    "    jointnet_output_dim = 2 if num_gaussians == 1 else num_gaussians * 2 + num_gaussians\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_config(json_path):\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config(config_dict, config_class):\n",
    "\n",
    "    for key, value in config_dict.items():\n",
    "        if hasattr(config_class, key):\n",
    "            setattr(config_class, key, value)\n",
    "        else:\n",
    "            print(f\"Warning: '{key}' not found in the class. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args_path = \"runs/exp_10/run_args.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args = load_json_config(run_args_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_config(run_args, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HyperNet and MainNet\n",
    "hypernet = HyperNet(cfg)\n",
    "mainnet = MainNet(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAE\\AppData\\Local\\Temp\\ipykernel_19372\\1268630429.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hypernet.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyperNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=6, out_features=1024, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (projection): MultiHeadLinearProjection(\n",
       "    (linears): ModuleList(\n",
       "      (0-1): 2 x ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=38400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=150, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=38400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=150, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=38400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=150, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (12): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (13): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (14): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=38400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (15): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=150, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (16): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (17): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (18): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=38400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (19): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=150, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (20): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (21): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (22): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=38400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (23): ProjectionHead(\n",
       "        (head): Sequential(\n",
       "          (linear_final): Linear(in_features=128, out_features=150, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model weights into HyperNet\n",
    "model_path = \"runs/exp_10/best_model.pt\"\n",
    "hypernet.load_state_dict(torch.load(model_path))\n",
    "hypernet.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hypernet = hypernet.to(device)\n",
    "mainnet = mainnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAE\\Desktop\\Organisatorisches\\FH\\AI\\3. Semester\\ROBOTICS\\robotics_project\\.venv\\Lib\\site-packages\\ikpy\\chain.py:60: UserWarning: Link Base link (index: 0) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "c:\\Users\\SAE\\Desktop\\Organisatorisches\\FH\\AI\\3. Semester\\ROBOTICS\\robotics_project\\.venv\\Lib\\site-packages\\ikpy\\chain.py:60: UserWarning: Link ee_fixed_joint (index: 7) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n"
     ]
    }
   ],
   "source": [
    "# Load the kinematic chain for FK calculations (optional)\n",
    "r_arm = ikpy.chain.Chain.from_urdf_file(cfg.chain_path)\n",
    "\n",
    "# Extract joint limits\n",
    "upper = []\n",
    "lower = []\n",
    "for i in range(1, len(r_arm.links) - 1):\n",
    "    lower.append(r_arm.links[i].bounds[0])\n",
    "    upper.append(r_arm.links[i].bounds[1])\n",
    "\n",
    "upper = np.array(upper)\n",
    "lower = np.array(lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input position (e.g., desired end-effector position)\n",
    "\n",
    "positions = torch.tensor([[0.009839, -0.123490, 0.046488, 0.448670, 0.547631, -0.706255]], dtype=torch.float32)  # first 3 are positions (TCP), last 3 are orientations \n",
    "positions = positions.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict weights using HyperNet\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    predicted_weights = hypernet(positions)\n",
    "\n",
    "# Generate joint angles using MainNet\n",
    "with torch.no_grad():\n",
    "    initial_input = torch.ones((positions.shape[0], 1), dtype=torch.float32).to(device)\n",
    "    samples, distributions, means, variance, selection = mainnet.validate(\n",
    "        initial_input, predicted_weights, lower, upper\n",
    "    )\n",
    "\n",
    "# Convert the predicted joint angles to a readable format\n",
    "predicted_joint_angles = []\n",
    "for sample in samples:\n",
    "    predicted_joint_angles.append([angle.item() for angle in sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.54919695854187],\n",
       " [0.29687026143074036],\n",
       " [2.5],\n",
       " [0.31112515926361084],\n",
       " [-2.040735960006714],\n",
       " [1.6099227666854858]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_joint_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predicted joint angles\n",
    "flat_joint_angles = [angle[0] for angle in predicted_joint_angles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add base and end-effector placeholders (if necessary)\n",
    "full_joint_angles = [0] + flat_joint_angles + [0]  # Base and end-effector placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 3.032597541809082,\n",
       " 0.6473861932754517,\n",
       " -2.5,\n",
       " -3.1415927410125732,\n",
       " -0.46253931522369385,\n",
       " 0.728458046913147,\n",
       " 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_joint_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_arm.links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check for length mismatch\n",
    "if len(full_joint_angles) != len(r_arm.links):\n",
    "    raise ValueError(\"Mismatch between joint angles and kinematic chain.\")\n",
    "\n",
    "# Compute Forward Kinematics\n",
    "fk_position = r_arm.forward_kinematics(full_joint_angles)[:3, 3]  # Extract end-effector position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FK Position using predicted joints: [-0.14791288 -0.16770742  0.21856937]\n",
      "original position: tensor([ 0.0098, -0.1235,  0.0465,  0.4487,  0.5476, -0.7063], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"FK Position using predicted joints: {fk_position}\")\n",
    "print(f\"original position: {positions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_parameters(model):\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    return {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"non_trainable_params\": total_params - trainable_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_params': 32966916,\n",
       " 'trainable_params': 32966916,\n",
       " 'non_trainable_params': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = count_model_parameters(hypernet)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_csv import IKDatasetValCSV\n",
    "dataloader = IKDatasetValCSV(\"ur5_val_data_87k copy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(hypernet, mainnet, dataloader, r_arm, lower, upper, threshold_position=0.1):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    hypernet.eval()\n",
    "    mainnet.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for positions, joint_angles in dataloader:\n",
    "            positions = positions.to(device).view(-1, positions.shape[-1])\n",
    "            joint_angles = joint_angles.to(device).view(-1, joint_angles.shape[-1])\n",
    "\n",
    "            # Split positions into positional (x, y, z) and orientation (rx, ry, rz) components\n",
    "            pos_xyz = positions[:, :3]  # x, y, z\n",
    "            ori_xyz = positions[:, 3:]  # rx, ry, rz\n",
    "\n",
    "            # Predict mixture weights with hypernet using the full position + orientation input\n",
    "            predicted_weights = hypernet(positions)\n",
    "\n",
    "            # Generate joint angle predictions using mainnet\n",
    "            initial_input = torch.ones((positions.shape[0], 1), dtype=torch.float32).to(device)\n",
    "            samples, _, _, _, _ = mainnet.validate(initial_input, predicted_weights, lower, upper)\n",
    "\n",
    "            for i in range(len(pos_xyz)):\n",
    "                # Construct joint angles for forward kinematics\n",
    "                flat_joint_angles = [sample[i].item() for sample in samples]\n",
    "                full_joint_angles = [0] + flat_joint_angles + [0]  # Add base and end-effector placeholders\n",
    "\n",
    "                # Compute FK for position\n",
    "                fk_matrix = r_arm.forward_kinematics(full_joint_angles)\n",
    "                fk_position = fk_matrix[:3, 3]\n",
    "\n",
    "                # Compare with target end-effector position\n",
    "                target_position = pos_xyz[i].cpu().numpy()\n",
    "                error_position = np.linalg.norm(fk_position - target_position)\n",
    "\n",
    "                # Count as correct if position is within threshold\n",
    "                if error_position < threshold_position:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    accuracy = correct / total * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy: {accuracy:.2f}% (position threshold: {threshold_position} meters)\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.25% (position threshold: 0.1 meters)\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(hypernet, mainnet, dataloader, r_arm, lower, upper) # IK8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
